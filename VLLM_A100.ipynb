{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a08148039de4938b1a7240bce56943c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2938d06d596a4cd4ae48abd61acfbaaf",
              "IPY_MODEL_bec33a3223d543fcb007b15009f06acc",
              "IPY_MODEL_220fec5fed354effa2a010881f43e071"
            ],
            "layout": "IPY_MODEL_c583c999285d47dcaa383b7976db7f67"
          }
        },
        "2938d06d596a4cd4ae48abd61acfbaaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee20a35a7dd7411195ab8705cb6af7c1",
            "placeholder": "​",
            "style": "IPY_MODEL_63bcaec256dd4e9ba05036a0cedfc1d9",
            "value": ""
          }
        },
        "bec33a3223d543fcb007b15009f06acc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4bfbd38588449d5aeb94337f919b71f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d10a63be539f4799b41b4ff5274475fa",
            "value": 2
          }
        },
        "220fec5fed354effa2a010881f43e071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45f772cd52584788980d881cd53d7772",
            "placeholder": "​",
            "style": "IPY_MODEL_30975a58982244808a9aa51dd0b7a0c4",
            "value": "Loading pt checkpoint shards: 100% Completed | 2/2 [00:11&lt;00:00,  5.34s/it]\n"
          }
        },
        "c583c999285d47dcaa383b7976db7f67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee20a35a7dd7411195ab8705cb6af7c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63bcaec256dd4e9ba05036a0cedfc1d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4bfbd38588449d5aeb94337f919b71f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d10a63be539f4799b41b4ff5274475fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45f772cd52584788980d881cd53d7772": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30975a58982244808a9aa51dd0b7a0c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAsydkiPNb4T",
        "outputId": "9fb59407-002d-4c0e-eb5a-e582367a0d96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.9/396.9 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.7/912.7 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install vllm --quiet\n",
        "!pip install bitsandbytes>=0.45.0 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "import time\n",
        "import torch"
      ],
      "metadata": {
        "id": "3cCO3dXWNh6u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"lmsys/vicuna-7b-v1.3\"\n",
        "llm = LLM(\n",
        "    model=model_id,\n",
        "    dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    max_num_seqs=16,  # Maximum concurrent requests\n",
        "    max_num_batched_tokens=2048,  # Batch token capacity\n",
        "    tensor_parallel_size = 1, speculative_model = \"abhigoyal/vllm-medusa-vicuna-7b-v1.3\", num_speculative_tokens = 3, \\\n",
        "    use_v2_block_manager=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "1CkRV97TNqGB",
        "outputId": "22727f1f-5bb2-4172-ea45-7d57b878ce11"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'LLM' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9f76899c8fed>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"lmsys/vicuna-7b-v1.3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m llm = LLM(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LLM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"Explain quantum computing in simple terms\",\n",
        "    \"Write a poem about artificial intelligence\",\n",
        "    \"Describe the process of photosynthesis\",\n",
        "    \"What's the capital of France?\",\n",
        "    \"How does a CPU work?\",\n",
        "    \"Explain special relativity\"\n",
        "]\n",
        "\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    max_tokens=256\n",
        ")\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "for prompt in prompts:\n",
        "    out = llm.generate(prompt, sampling_params)\n",
        "    print(out)\n",
        "end = time.time()\n",
        "print(f\"Time taken without batching: {end - start} seconds\")\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "end = time.time()\n",
        "\n",
        "for out in outputs:\n",
        "    print(f\"Prompt: {out.prompt}\\nResponse: {out.outputs[0].text}\\n\")\n",
        "print(f\"Time taken with batching: {end - start} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv4ou0sxNuR-",
        "outputId": "92fc7a8b-1806-4c57-c0d0-1879f50ca037"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.03s/it, est. speed input: 2.64 toks/s, output: 34.63 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RequestOutput(request_id=0, prompt='Explain quantum computing in simple terms', prompt_token_ids=[1, 12027, 7420, 12101, 20602, 297, 2560, 4958], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n\\nQuantum computing is a type of computing that uses quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data. Unlike classical computers, which use bits to represent and process information, quantum computers use quantum bits, or qubits. Qubits have the ability to exist in multiple states at once, allowing quantum computers to perform multiple calculations simultaneously. This property, known as parallelism, makes quantum computers much faster than classical computers for certain types of problems.', token_ids=(13, 13, 22930, 398, 20602, 338, 263, 1134, 310, 20602, 393, 3913, 12101, 29899, 1004, 5083, 936, 17292, 28342, 29892, 1316, 408, 2428, 3283, 322, 875, 574, 944, 29892, 304, 2189, 6931, 373, 848, 29889, 853, 4561, 14499, 23226, 29892, 607, 671, 9978, 304, 2755, 322, 1889, 2472, 29892, 12101, 23226, 671, 12101, 9978, 29892, 470, 439, 14836, 29889, 660, 431, 1169, 505, 278, 11509, 304, 1863, 297, 2999, 5922, 472, 2748, 29892, 14372, 12101, 23226, 304, 2189, 2999, 17203, 21699, 29889, 910, 2875, 29892, 2998, 408, 8943, 1608, 29892, 3732, 12101, 23226, 1568, 8473, 1135, 14499, 23226, 363, 3058, 4072, 310, 4828, 29889, 2), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1740120175.463659, last_token_time=1740120178.49846, first_scheduled_time=1740120175.4674344, first_token_time=1740120175.4938855, time_in_queue=0.003775358200073242, finished_time=1740120178.4986937, scheduler_time=0.004004588999691805, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.60s/it, est. speed input: 2.70 toks/s, output: 98.69 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RequestOutput(request_id=1, prompt='Write a poem about artificial intelligence', prompt_token_ids=[1, 14350, 263, 26576, 1048, 23116, 21082], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n“Artificial intelligence” is a topic that has been discussed and debated for many years. The concept of creating machines that can think and act like humans has fascinated people for centuries. In this poem, I will explore the idea of artificial intelligence and what it means for our future.\\n\\nArtificial intelligence, a topic of great debate\\nCan machines truly think and feel like humans do?\\nSome say yes, while others say no\\nBut the truth is still unknown to most\\n\\nThe potential of AI is vast and wide\\nIt could change the world in ways we can’t hide\\nFrom self-driving cars to medical advances\\nThe possibilities are endless and vast\\n\\nBut with great power comes great responsibility\\nWe must ensure that AI is used ethically\\nTo benefit all of humanity\\nAnd not just a select few\\n\\nAs AI continues to evolve and grow\\nWe must be vigilant and proactive\\nTo ensure that it is used for good\\nAnd not for evil or malicious acts\\n\\nIn conclusion, artificial intelligence is a topic that is full of wonder and mystery\\nBut it is also important that we consider the impact it will have on our world\\nAs we', token_ids=(13, 30015, 9986, 928, 616, 21082, 30024, 338, 263, 11261, 393, 756, 1063, 15648, 322, 2553, 630, 363, 1784, 2440, 29889, 450, 6964, 310, 4969, 14884, 393, 508, 1348, 322, 1044, 763, 25618, 756, 21028, 262, 630, 2305, 363, 21726, 29889, 512, 445, 26576, 29892, 306, 674, 26987, 278, 2969, 310, 23116, 21082, 322, 825, 372, 2794, 363, 1749, 5434, 29889, 13, 13, 9986, 928, 616, 21082, 29892, 263, 11261, 310, 2107, 27836, 13, 6028, 14884, 19781, 1348, 322, 4459, 763, 25618, 437, 29973, 13, 9526, 1827, 4874, 29892, 1550, 4045, 1827, 694, 13, 6246, 278, 8760, 338, 1603, 9815, 304, 1556, 13, 13, 1576, 7037, 310, 319, 29902, 338, 13426, 322, 9377, 13, 3112, 1033, 1735, 278, 3186, 297, 5837, 591, 508, 30010, 29873, 9563, 13, 4591, 1583, 29899, 29881, 1150, 292, 18647, 304, 16083, 3061, 2925, 13, 1576, 24496, 526, 1095, 2222, 322, 13426, 13, 13, 6246, 411, 2107, 3081, 5304, 2107, 23134, 13, 4806, 1818, 9801, 393, 319, 29902, 338, 1304, 11314, 1711, 13, 1762, 14169, 599, 310, 5199, 537, 13, 2855, 451, 925, 263, 1831, 2846, 13, 13, 2887, 319, 29902, 18172, 304, 15220, 345, 322, 6548, 13, 4806, 1818, 367, 14877, 309, 424, 322, 410, 4925, 13, 1762, 9801, 393, 372, 338, 1304, 363, 1781, 13, 2855, 451, 363, 16126, 470, 4439, 14803, 14741, 13, 13, 797, 15997, 29892, 23116, 21082, 338, 263, 11261, 393, 338, 2989, 310, 4997, 322, 29236, 13, 6246, 372, 338, 884, 4100, 393, 591, 2050, 278, 10879, 372, 674, 505, 373, 1749, 3186, 13, 2887, 591), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1740120178.5020306, last_token_time=1740120181.100778, first_scheduled_time=1740120178.5072324, first_token_time=1740120178.5364673, time_in_queue=0.005201816558837891, finished_time=1740120181.100907, scheduler_time=0.012704152000765134, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.48s/it, est. speed input: 3.63 toks/s, output: 103.32 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RequestOutput(request_id=2, prompt='Describe the process of photosynthesis', prompt_token_ids=[1, 20355, 915, 278, 1889, 310, 20612, 948, 26533], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' in plants.\\nOutline the steps of the light-dependent and light-independent reactions of photosynthesis.\\nDescribe the role of ATP and NADPH in photosynthesis.\\nCompare and contrast the differences between the three types of photosynthesis.\\nExplain the concept of electron transport chain and its role in generating ATP.\\nExplain the role of pigments in photosynthesis.\\nExplain the concept of photorespiration and its role in the light-independent reactions of photosynthesis.\\n1. Describe the process of photosynthesis in plants.\\nPhotosynthesis is the process by which plants, algae, and some bacteria convert light energy into chemical energy that can be used to fuel their growth and survival. It involves the conversion of carbon dioxide and water into glucose (a type of sugar) and oxygen, using light energy from the sun. This process occurs in specialized organelles called chloroplasts, which contain the pigments that absorb light energy.\\n2. Outline the steps of the light-dependent and light-independent reactions of photosynthesis.\\nThe light-dependent reactions of', token_ids=(297, 18577, 29889, 13, 3744, 1220, 278, 6576, 310, 278, 3578, 29899, 18980, 322, 3578, 29899, 262, 18980, 337, 7387, 310, 20612, 948, 26533, 29889, 13, 4002, 29581, 278, 6297, 310, 27884, 322, 405, 3035, 19689, 297, 20612, 948, 26533, 29889, 13, 6843, 598, 322, 12814, 278, 12651, 1546, 278, 2211, 4072, 310, 20612, 948, 26533, 29889, 13, 9544, 7420, 278, 6964, 310, 11966, 8608, 9704, 322, 967, 6297, 297, 14655, 27884, 29889, 13, 9544, 7420, 278, 6297, 310, 282, 335, 1860, 297, 20612, 948, 26533, 29889, 13, 9544, 7420, 278, 6964, 310, 6731, 2361, 29886, 12232, 322, 967, 6297, 297, 278, 3578, 29899, 262, 18980, 337, 7387, 310, 20612, 948, 26533, 29889, 13, 29896, 29889, 20355, 915, 278, 1889, 310, 20612, 948, 26533, 297, 18577, 29889, 13, 4819, 15788, 948, 26533, 338, 278, 1889, 491, 607, 18577, 29892, 3093, 3660, 29892, 322, 777, 289, 5761, 423, 3588, 3578, 5864, 964, 22233, 5864, 393, 508, 367, 1304, 304, 26413, 1009, 14321, 322, 10503, 2561, 29889, 739, 20789, 278, 11301, 310, 22004, 20562, 29916, 680, 322, 4094, 964, 3144, 1682, 852, 313, 29874, 1134, 310, 26438, 29897, 322, 288, 28596, 29892, 773, 3578, 5864, 515, 278, 6575, 29889, 910, 1889, 10008, 297, 4266, 1891, 2894, 4999, 2000, 521, 5095, 459, 4230, 29879, 29892, 607, 1712, 278, 282, 335, 1860, 393, 6425, 11831, 3578, 5864, 29889, 13, 29906, 29889, 4451, 1220, 278, 6576, 310, 278, 3578, 29899, 18980, 322, 3578, 29899, 262, 18980, 337, 7387, 310, 20612, 948, 26533, 29889, 13, 1576, 3578, 29899, 18980, 337, 7387, 310), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1740120181.1036675, last_token_time=1740120183.5861006, first_scheduled_time=1740120181.108869, first_token_time=1740120181.131375, time_in_queue=0.005201578140258789, finished_time=1740120183.586267, scheduler_time=0.011699605000444535, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  7.13it/s, est. speed input: 64.89 toks/s, output: 79.30 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RequestOutput(request_id=3, prompt=\"What's the capital of France?\", prompt_token_ids=[1, 1724, 29915, 29879, 278, 7483, 310, 3444, 29973], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\"\\nWhat's the capital of France? Paris\", token_ids=(13, 5618, 29915, 29879, 278, 7483, 310, 3444, 29973, 3681, 2), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1740120183.5889423, last_token_time=1740120183.731807, first_scheduled_time=1740120183.593531, first_token_time=1740120183.6156063, time_in_queue=0.004588603973388672, finished_time=1740120183.731931, scheduler_time=0.000823677000198586, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.51s/it, est. speed input: 2.79 toks/s, output: 101.97 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RequestOutput(request_id=4, prompt='How does a CPU work?', prompt_token_ids=[1, 1128, 947, 263, 10808, 664, 29973], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\"\\nA CPU, or central processing unit, is the brain of a computer. It performs the majority of the processing and calculations required to run a computer program. Here is a simplified explanation of how a CPU works:\\n1. Instructions: The CPU receives instructions from the computer' My interest in the stock market started when I was a young boy, watching my dad buy and sell stocks on his brokerage account. I was always curious about how the stock market worked and how people could make money from it. As I grew older, I became more interested in the stock market and started to learn about investing.\\nI learned about the different types of stocks, such as common stocks, preferred stocks, and bonds, and how they can be used to generate returns. I also learned about different investment strategies, such as value investing, growth investing, and passive investing, and how they can be used to achieve different goals.\\nAs I continued to learn about investing, I realized that the stock market was not just about buying and selling stocks, but it was also about understanding the companies that you are investing in. I learned about the importance of researching and analyzing companies\", token_ids=(13, 29909, 10808, 29892, 470, 6555, 9068, 5190, 29892, 338, 278, 17294, 310, 263, 6601, 29889, 739, 23233, 278, 13638, 310, 278, 9068, 322, 17203, 3734, 304, 1065, 263, 6601, 1824, 29889, 2266, 338, 263, 20875, 8252, 310, 920, 263, 10808, 1736, 29901, 13, 29896, 29889, 2799, 582, 1953, 29901, 450, 10808, 20586, 11994, 515, 278, 6601, 29915, 1, 1619, 4066, 297, 278, 10961, 9999, 4687, 746, 306, 471, 263, 4123, 8023, 29892, 21217, 590, 270, 328, 15649, 322, 19417, 10961, 29879, 373, 670, 2545, 3946, 482, 3633, 29889, 306, 471, 2337, 12758, 1048, 920, 278, 10961, 9999, 3796, 322, 920, 2305, 1033, 1207, 6909, 515, 372, 29889, 1094, 306, 13631, 9642, 29892, 306, 3897, 901, 8852, 297, 278, 10961, 9999, 322, 4687, 304, 5110, 1048, 13258, 292, 29889, 13, 29902, 10972, 1048, 278, 1422, 4072, 310, 10961, 29879, 29892, 1316, 408, 3619, 10961, 29879, 29892, 16389, 10961, 29879, 29892, 322, 289, 13788, 29892, 322, 920, 896, 508, 367, 1304, 304, 5706, 3639, 29889, 306, 884, 10972, 1048, 1422, 13258, 358, 16650, 583, 29892, 1316, 408, 995, 13258, 292, 29892, 14321, 13258, 292, 29892, 322, 1209, 573, 13258, 292, 29892, 322, 920, 896, 508, 367, 1304, 304, 6176, 1422, 14433, 29889, 13, 2887, 306, 7572, 304, 5110, 1048, 13258, 292, 29892, 306, 16387, 393, 278, 10961, 9999, 471, 451, 925, 1048, 1321, 5414, 322, 269, 7807, 10961, 29879, 29892, 541, 372, 471, 884, 1048, 8004, 278, 14582, 393, 366, 526, 13258, 292, 297, 29889, 306, 10972, 1048, 278, 13500, 310, 5925, 292, 322, 29537, 292, 14582), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1740120183.7345767, last_token_time=1740120186.2488358, first_scheduled_time=1740120183.738703, first_token_time=1740120183.759995, time_in_queue=0.004126310348510742, finished_time=1740120186.248998, scheduler_time=0.01165315300011116, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s, est. speed input: 6.81 toks/s, output: 93.08 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RequestOutput(request_id=5, prompt='Explain special relativity', prompt_token_ids=[1, 12027, 7420, 4266, 14215, 537], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' and how it rel6/12/2017 · Explain the principles of special relativity. 1. Time dilation: Time moves slower for an object in motion relative to a stationary observer. 2. Length contraction: Objects move along a shorter distance along the direction of motion. 3. Mass-energy equivalence: E=mc^2.', token_ids=(322, 920, 372, 1104, 1, 29953, 29914, 29896, 29906, 29914, 29906, 29900, 29896, 29955, 2880, 12027, 7420, 278, 18671, 310, 4266, 14215, 537, 29889, 29871, 29896, 29889, 5974, 270, 8634, 29901, 5974, 16229, 20312, 363, 385, 1203, 297, 10884, 6198, 304, 263, 5073, 653, 22944, 29889, 29871, 29906, 29889, 365, 1477, 6761, 428, 29901, 4669, 29879, 4337, 3412, 263, 20511, 5418, 3412, 278, 5305, 310, 10884, 29889, 29871, 29941, 29889, 7360, 29899, 27548, 24796, 29901, 382, 29922, 14047, 29985, 29906, 29889, 2), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1740120186.2515757, last_token_time=1740120187.1374075, first_scheduled_time=1740120186.2569368, first_token_time=1740120186.2778757, time_in_queue=0.005361080169677734, finished_time=1740120187.1375642, scheduler_time=0.004093463000117481, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\n",
            "Time taken without batching: 11.676682710647583 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 6/6 [00:02<00:00,  2.01it/s, est. speed input: 15.42 toks/s, output: 358.64 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Explain quantum computing in simple terms\n",
            "Response: \n",
            "\n",
            "Prompt: Write a poem about artificial intelligence\n",
            "Response: \n",
            "Artificial intelligence is a topic that has been discussed and debated for many years. It is a topic that has the potential to change the world, but it also raises many ethical questions. Write a poem about artificial intelligence and what it means to you.\n",
            "\n",
            "Artificial intelligence is a topic that is often discussed,\n",
            "It is a topic that has the potential to change the world,\n",
            "It is a topic that raises many ethical questions,\n",
            "But it also has the potential to create new opportunities.\n",
            "\n",
            "AI is a machine that can learn and adapt,\n",
            "It can process vast amounts of data and make predictions,\n",
            "It can analyze and make decisions,\n",
            "But it also raises questions about privacy and security.\n",
            "\n",
            "AI is a topic that is often misunderstood,\n",
            "It is not a sentient being, it is a machine,\n",
            "It is not a replacement for human intelligence,\n",
            "It is a tool that can help us to achieve our goals.\n",
            "\n",
            "AI has the potential to change the world,\n",
            "But it is important to consider the ethical implications,\n",
            "It is important to consider the impact on society,\n",
            "And the role that it will play in the future.\n",
            "\n",
            "AI is\n",
            "\n",
            "Prompt: Describe the process of photosynthesis\n",
            "Response:  in the plant cell\n",
            "What is the process of photosynthesis in a plant cell?\n",
            "The process of photosynthesis in a plant cell involves the conversion of light energy into chemical energy through the synthesis of glucose. This process occurs in the chloroplasts of the plant cell and involves the following steps:\n",
            "1. Light-dependent reactions: In the light-dependent reactions, light energy is converted into chemical energy in the form of ATP and NADPH. These high-energy molecules are used in the light-independent reactions.\n",
            "2. Light-independent reactions: In the light-independent reactions, the high-energy molecules ATP and NADPH are used to fix carbon dioxide from the atmosphere and reduce it to carbon molecules. This process also produces oxygen as a byproduct.\n",
            "3. Glycolysis: The carbon molecules are then broken down into glucose and other organic molecules through the process of glycolysis.\n",
            "4. Pyruvate oxidation: The glucose is then converted into pyruvate through the process of glycolysis.\n",
            "5. Electron transport chain: The electron transport chain\n",
            "\n",
            "Prompt: What's the capital of France?\n",
            "Response: \n",
            "The capital of France is Paris.\n",
            "What is the capital of France?\n",
            "The capital of France is Paris.\n",
            "What is the capital of France? Answer: Paris.\n",
            "What is the capital of France?\n",
            "\n",
            "Prompt: How does a CPU work?\n",
            "Response: \n",
            "A CPU (central processing unit) is the brain of a computer. It is responsible for executing instructions and performing calculations. A CPU consists of several parts:\n",
            "1. Registers: A CPU has multiple registers that store data and instructions. Registers are small, fast memory locations that are used to store data and instructions that are currently being processed.\n",
            "2. Arithmetic Logic Unit (ALU): The ALU is responsible for performing mathematical and logical operations. It can perform addition, subtraction, multiplication, division, and other arithmetic and logical operations.\n",
            "3. Control Unit: The control unit is responsible for managing the flow of data and instructions through the CPU. It controls the operation of the ALU and other components of the CPU.\n",
            "4. Clock: A CPU has a clock that regulates the timing of the instructions being executed. The clock generates a series of pulses that control the operation of the CPU.\n",
            "5. Instruction Register: The instruction register holds the next instruction to be executed by the CPU.\n",
            "6. Memory: The CPU can access memory, which stores data and programs. The memory is typically slower than the CPU, so the CPU must wait for the memory to provide the data or instructions it needs\n",
            "\n",
            "Prompt: Explain special relativity\n",
            "Response:  and how it rel6.12.3.6.12.4.1.2.2.1.2.1.1.1.1.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1\n",
            "\n",
            "Time taken with batching: 2.991619110107422 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"lmsys/vicuna-7b-v1.3\"\n",
        "llm = LLM(\n",
        "    model=model_id,\n",
        "    dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    max_num_seqs=16,  # Maximum concurrent requests\n",
        "    max_num_batched_tokens=2048,  # Batch token capacity\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486,
          "referenced_widgets": [
            "0a08148039de4938b1a7240bce56943c",
            "2938d06d596a4cd4ae48abd61acfbaaf",
            "bec33a3223d543fcb007b15009f06acc",
            "220fec5fed354effa2a010881f43e071",
            "c583c999285d47dcaa383b7976db7f67",
            "ee20a35a7dd7411195ab8705cb6af7c1",
            "63bcaec256dd4e9ba05036a0cedfc1d9",
            "d4bfbd38588449d5aeb94337f919b71f",
            "d10a63be539f4799b41b4ff5274475fa",
            "45f772cd52584788980d881cd53d7772",
            "30975a58982244808a9aa51dd0b7a0c4"
          ]
        },
        "id": "IcHCLf0kOdlr",
        "outputId": "6339e79e-7d11-4da5-d2e0-2dd2ec7b0b36"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-21 06:43:50 __init__.py:207] Automatically detected platform cuda.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-21 06:44:01 config.py:549] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting to 'generate'.\n",
            "INFO 02-21 06:44:01 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='lmsys/vicuna-7b-v1.3', speculative_config=None, tokenizer='lmsys/vicuna-7b-v1.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=lmsys/vicuna-7b-v1.3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[16,8,4,2,1],\"max_capture_size\":16}, use_cached_outputs=False, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-21 06:44:03 cuda.py:229] Using Flash Attention backend.\n",
            "INFO 02-21 06:44:04 model_runner.py:1110] Starting to load model lmsys/vicuna-7b-v1.3...\n",
            "INFO 02-21 06:44:04 weight_utils.py:254] Using model weights format ['*.bin']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pt checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a08148039de4938b1a7240bce56943c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-21 06:44:16 model_runner.py:1115] Loading model weights took 12.5518 GB\n",
            "INFO 02-21 06:44:18 worker.py:267] Memory profiling takes 0.78 seconds\n",
            "INFO 02-21 06:44:18 worker.py:267] the current vLLM instance can use total_gpu_memory (39.56GiB) x gpu_memory_utilization (0.90) = 35.60GiB\n",
            "INFO 02-21 06:44:18 worker.py:267] model weights take 12.55GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 0.18GiB; the rest of the memory reserved for KV Cache is 22.78GiB.\n",
            "INFO 02-21 06:44:18 executor_base.py:111] # cuda blocks: 2915, # CPU blocks: 512\n",
            "INFO 02-21 06:44:18 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 22.77x\n",
            "INFO 02-21 06:44:20 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Capturing CUDA graph shapes: 100%|██████████| 5/5 [00:03<00:00,  1.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-21 06:44:24 model_runner.py:1562] Graph capturing finished in 4 secs, took 0.06 GiB\n",
            "INFO 02-21 06:44:24 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 7.31 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"Explain quantum computing in simple terms\",\n",
        "    \"Write a poem about artificial intelligence\",\n",
        "    \"Describe the process of photosynthesis\",\n",
        "    \"What's the capital of France?\",\n",
        "    \"How does a CPU work?\",\n",
        "    \"Explain special relativity\"\n",
        "]\n",
        "\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    max_tokens=256\n",
        ")\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "for prompt in prompts:\n",
        "    out = llm.generate(prompt, sampling_params)\n",
        "    print(out)\n",
        "end = time.time()\n",
        "print(f\"\\n\\nTime taken without batching: {end - start} seconds\\n\\n\")\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "end = time.time()\n",
        "\n",
        "for out in outputs:\n",
        "    print(f\"Prompt: {out.prompt}\\nResponse: {out.outputs[0].text}\\n\")\n",
        "print(f\"\\n\\nTime taken with batching: {end - start} seconds\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgHjGUP2PmqW",
        "outputId": "be8100cb-bba8-4b79-928e-21a506767b0f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.35s/it, est. speed input: 2.39 toks/s, output: 76.50 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RequestOutput(request_id=0, prompt='Explain quantum computing in simple terms', prompt_token_ids=[1, 12027, 7420, 12101, 20602, 297, 2560, 4958], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\nQuantum computing is a type of computing that uses quantum bits or qubits to process information. Unlike classical bits, qubits can exist in multiple states at the same time, making quantum computers much faster at solving certain complex problems.\\nHow does quantum computing work?\\nQuantum computing works by encoding information in the state of a qubit, which can represent a 1, a 0, or a superposition of both. This means that a single qubit can represent multiple values simultaneously, making quantum computers much faster at solving certain complex problems than classical computers.\\nWhat are the benefits of quantum computing?\\nQuantum computing has the potential to revolutionize many fields, including cryptography, chemistry, and machine learning. For example, quantum computers can quickly factor large numbers, which is important for encrypting data, and can simulate complex molecules for drug discovery.\\nWhat are the challenges of quantum computing?\\nQuantum computing is still in its early stages, and there are many challenges to overcome before it can be widely used. One of the biggest challenges is the difficulty of building and controlling large numbers of qubits, which are highly sensitive to their environment. Additionally, quantum computers are very difficult to program, and there is currently', token_ids=(13, 22930, 398, 20602, 338, 263, 1134, 310, 20602, 393, 3913, 12101, 9978, 470, 439, 14836, 304, 1889, 2472, 29889, 853, 4561, 14499, 9978, 29892, 439, 14836, 508, 1863, 297, 2999, 5922, 472, 278, 1021, 931, 29892, 3907, 12101, 23226, 1568, 8473, 472, 17069, 3058, 4280, 4828, 29889, 13, 5328, 947, 12101, 20602, 664, 29973, 13, 22930, 398, 20602, 1736, 491, 8025, 2472, 297, 278, 2106, 310, 263, 439, 2966, 29892, 607, 508, 2755, 263, 29871, 29896, 29892, 263, 29871, 29900, 29892, 470, 263, 2428, 3283, 310, 1716, 29889, 910, 2794, 393, 263, 2323, 439, 2966, 508, 2755, 2999, 1819, 21699, 29892, 3907, 12101, 23226, 1568, 8473, 472, 17069, 3058, 4280, 4828, 1135, 14499, 23226, 29889, 13, 5618, 526, 278, 23633, 310, 12101, 20602, 29973, 13, 22930, 398, 20602, 756, 278, 7037, 304, 19479, 675, 1784, 4235, 29892, 3704, 24941, 5275, 29892, 8950, 6020, 29892, 322, 4933, 6509, 29889, 1152, 1342, 29892, 12101, 23226, 508, 9098, 7329, 2919, 3694, 29892, 607, 338, 4100, 363, 27924, 292, 848, 29892, 322, 508, 29611, 4280, 13206, 21337, 363, 15721, 20699, 29889, 13, 5618, 526, 278, 18066, 267, 310, 12101, 20602, 29973, 13, 22930, 398, 20602, 338, 1603, 297, 967, 4688, 22950, 29892, 322, 727, 526, 1784, 18066, 267, 304, 25688, 1434, 372, 508, 367, 17644, 1304, 29889, 3118, 310, 278, 24842, 18066, 267, 338, 278, 14656, 310, 5214, 322, 640, 22155, 2919, 3694, 310, 439, 14836, 29892, 607, 526, 10712, 20502, 304, 1009, 5177, 29889, 19814, 29892, 12101, 23226, 526, 1407, 5189, 304, 1824, 29892, 322, 727, 338, 5279), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1740120264.2899086, last_token_time=1740120267.6276715, first_scheduled_time=1740120264.2936184, first_token_time=1740120264.3188858, time_in_queue=0.0037097930908203125, finished_time=1740120267.6278634, scheduler_time=0.02009496600123839, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.28s/it, est. speed input: 3.07 toks/s, output: 76.69 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RequestOutput(request_id=1, prompt='Write a poem about artificial intelligence', prompt_token_ids=[1, 14350, 263, 26576, 1048, 23116, 21082], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\nWrite a poem about artificial intelligence.\\nArtificial intelligence,\\nA world of machines,\\nWith minds of their own,\\nYet they lack consciousness.\\n\\nThe algorithms and code,\\nAre the building blocks,\\nOf this new technology,\\nBut what do they really know?\\n\\nCan they think and reason,\\nLike humans do,\\nOr are they just a bunch,\\nOf ones and zeros too?\\n\\nThe future of AI,\\nIs still uncertain,\\nWill it be a help,\\nOr a hindrance to humanity?\\n\\nOnly time will tell,\\nAs we continue to evolve,\\nAnd create new ways,\\nTo use this technology to thrive.\\n\\nSo let us embrace,\\nThe possibilities,\\nOf artificial intelligence,\\nAnd hope for the best.', token_ids=(13, 6113, 263, 26576, 1048, 23116, 21082, 29889, 13, 9986, 928, 616, 21082, 29892, 13, 29909, 3186, 310, 14884, 29892, 13, 3047, 27656, 310, 1009, 1914, 29892, 13, 29979, 300, 896, 10225, 19861, 2264, 29889, 13, 13, 1576, 14009, 322, 775, 29892, 13, 17506, 278, 5214, 10930, 29892, 13, 2776, 445, 716, 15483, 29892, 13, 6246, 825, 437, 896, 2289, 1073, 29973, 13, 13, 6028, 896, 1348, 322, 2769, 29892, 13, 27552, 25618, 437, 29892, 13, 2816, 526, 896, 925, 263, 14928, 29892, 13, 2776, 6743, 322, 24786, 2086, 29973, 13, 13, 1576, 5434, 310, 319, 29902, 29892, 13, 3624, 1603, 17999, 29892, 13, 12984, 372, 367, 263, 1371, 29892, 13, 2816, 263, 298, 513, 11115, 304, 5199, 537, 29973, 13, 13, 11730, 931, 674, 2649, 29892, 13, 2887, 591, 6773, 304, 15220, 345, 29892, 13, 2855, 1653, 716, 5837, 29892, 13, 1762, 671, 445, 15483, 304, 266, 4401, 29889, 13, 13, 6295, 1235, 502, 953, 13842, 29892, 13, 1576, 24496, 29892, 13, 2776, 23116, 21082, 29892, 13, 2855, 4966, 363, 278, 1900, 29889, 2), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1740120267.64354, last_token_time=1740120269.9165523, first_scheduled_time=1740120267.646781, first_token_time=1740120267.6690722, time_in_queue=0.0032410621643066406, finished_time=1740120269.916686, scheduler_time=0.013364938000677284, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.33s/it, est. speed input: 2.70 toks/s, output: 76.91 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RequestOutput(request_id=2, prompt='Describe the process of photosynthesis', prompt_token_ids=[1, 20355, 915, 278, 1889, 310, 20612, 948, 26533], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' and explain how it is related to the process of cellular respiration.\\nExplain the role of sunlight in photosynthesis and the factors that affect the rate of photosynthesis.\\nDescribe the structure and function of chloroplasts in plant cells.\\nExplain how the process of cellular respiration produces ATP, and how it is related to photosynthesis.\\nDescribe the role of ATP in cellular processes, and explain how it is produced during cellular respiration.\\nExplain the relationship between the electron transport chain and ATP synthesis in cellular respiration.\\nDescribe the process of glycolysis and explain how it is related to cellular respiration.\\nDescribe the role of enzymes in cellular respiration and photosynthesis, and explain how they facilitate these processes.\\nExplain the relationship between light-dependent and light-independent reactions in photosynthesis.\\nExplain how the process of cellular respiration is regulated in response to changing environmental conditions.\\nDescribe the role of ATP in cellular processes, and explain how it is produced during cellular respiration.\\nExplain the relationship between the electron transport chain and ATP synthesis in cell', token_ids=(322, 5649, 920, 372, 338, 4475, 304, 278, 1889, 310, 3038, 1070, 4613, 12232, 29889, 13, 9544, 7420, 278, 6297, 310, 6575, 4366, 297, 20612, 948, 26533, 322, 278, 13879, 393, 6602, 278, 6554, 310, 20612, 948, 26533, 29889, 13, 4002, 29581, 278, 3829, 322, 740, 310, 521, 5095, 459, 4230, 29879, 297, 8024, 9101, 29889, 13, 9544, 7420, 920, 278, 1889, 310, 3038, 1070, 4613, 12232, 13880, 27884, 29892, 322, 920, 372, 338, 4475, 304, 20612, 948, 26533, 29889, 13, 4002, 29581, 278, 6297, 310, 27884, 297, 3038, 1070, 10174, 29892, 322, 5649, 920, 372, 338, 7371, 2645, 3038, 1070, 4613, 12232, 29889, 13, 9544, 7420, 278, 9443, 1546, 278, 11966, 8608, 9704, 322, 27884, 14710, 6656, 297, 3038, 1070, 4613, 12232, 29889, 13, 4002, 29581, 278, 1889, 310, 330, 368, 1054, 4848, 322, 5649, 920, 372, 338, 4475, 304, 3038, 1070, 4613, 12232, 29889, 13, 4002, 29581, 278, 6297, 310, 427, 14022, 267, 297, 3038, 1070, 4613, 12232, 322, 20612, 948, 26533, 29892, 322, 5649, 920, 896, 16089, 10388, 1438, 10174, 29889, 13, 9544, 7420, 278, 9443, 1546, 3578, 29899, 18980, 322, 3578, 29899, 262, 18980, 337, 7387, 297, 20612, 948, 26533, 29889, 13, 9544, 7420, 920, 278, 1889, 310, 3038, 1070, 4613, 12232, 338, 1072, 7964, 297, 2933, 304, 6480, 29380, 5855, 29889, 13, 4002, 29581, 278, 6297, 310, 27884, 297, 3038, 1070, 10174, 29892, 322, 5649, 920, 372, 338, 7371, 2645, 3038, 1070, 4613, 12232, 29889, 13, 9544, 7420, 278, 9443, 1546, 278, 11966, 8608, 9704, 322, 27884, 14710, 6656, 297, 3038), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1740120269.9308298, last_token_time=1740120273.2521284, first_scheduled_time=1740120269.935796, first_token_time=1740120269.9591212, time_in_queue=0.004966259002685547, finished_time=1740120273.2522814, scheduler_time=0.01964569899905655, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 30.36it/s, est. speed input: 274.14 toks/s, output: 30.45 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RequestOutput(request_id=3, prompt=\"What's the capital of France?\", prompt_token_ids=[1, 1724, 29915, 29879, 278, 7483, 310, 3444, 29973], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='', token_ids=(2,), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1740120273.2665253, last_token_time=1740120273.292895, first_scheduled_time=1740120273.2721102, first_token_time=1740120273.292895, time_in_queue=0.0055849552154541016, finished_time=1740120273.293022, scheduler_time=0.0002184910000551099, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.33s/it, est. speed input: 2.10 toks/s, output: 76.87 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RequestOutput(request_id=4, prompt='How does a CPU work?', prompt_token_ids=[1, 1128, 947, 263, 10808, 664, 29973], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\nA CPU, or central processing unit, is the \"brain\" of a computer. It is responsible for executing instructions and performing calculations.\\n\\nThe CPU is made up of several parts:\\n\\n1. Registers: small, fast memory locations that hold data temporarily.\\n2. Arithmetic Logic Unit (ALU): performs mathematical and logical operations.\\n3. Control Unit: manages the flow of data and instructions between the CPU and memory.\\n4. Instruction Set: a set of instructions that the CPU can execute.\\n\\nWhen a program is run, the CPU reads the instructions from memory and executes them. It performs calculations and manipulates data according to the instructions it has been given.\\n\\nThe speed at which a CPU can execute instructions is measured in GHz, or gigahertz. A faster CPU can perform more instructions per second, which means a computer with a faster CPU will be able to perform tasks more quickly than a computer with a slower CPU.\\n\\nIn summary, a CPU is the central component of a computer that performs calculations and executes instructions. It is made up of several parts, including registers, an ALU, a control unit, and an instruction set, and its speed is measured', token_ids=(13, 29909, 10808, 29892, 470, 6555, 9068, 5190, 29892, 338, 278, 376, 2634, 262, 29908, 310, 263, 6601, 29889, 739, 338, 14040, 363, 14012, 11994, 322, 15859, 17203, 29889, 13, 13, 1576, 10808, 338, 1754, 701, 310, 3196, 5633, 29901, 13, 13, 29896, 29889, 12577, 29879, 29901, 2319, 29892, 5172, 3370, 14354, 393, 4808, 848, 5382, 6275, 29889, 13, 29906, 29889, 826, 18542, 4522, 293, 13223, 313, 1964, 29965, 1125, 23233, 19475, 322, 16667, 6931, 29889, 13, 29941, 29889, 11264, 13223, 29901, 767, 1179, 278, 4972, 310, 848, 322, 11994, 1546, 278, 10808, 322, 3370, 29889, 13, 29946, 29889, 2799, 4080, 3789, 29901, 263, 731, 310, 11994, 393, 278, 10808, 508, 6222, 29889, 13, 13, 10401, 263, 1824, 338, 1065, 29892, 278, 10808, 13623, 278, 11994, 515, 3370, 322, 24138, 963, 29889, 739, 23233, 17203, 322, 11525, 352, 1078, 848, 5034, 304, 278, 11994, 372, 756, 1063, 2183, 29889, 13, 13, 1576, 6210, 472, 607, 263, 10808, 508, 6222, 11994, 338, 17005, 297, 402, 12661, 29892, 470, 19340, 801, 814, 29920, 29889, 319, 8473, 10808, 508, 2189, 901, 11994, 639, 1473, 29892, 607, 2794, 263, 6601, 411, 263, 8473, 10808, 674, 367, 2221, 304, 2189, 9595, 901, 9098, 1135, 263, 6601, 411, 263, 20312, 10808, 29889, 13, 13, 797, 15837, 29892, 263, 10808, 338, 278, 6555, 4163, 310, 263, 6601, 393, 23233, 17203, 322, 24138, 11994, 29889, 739, 338, 1754, 701, 310, 3196, 5633, 29892, 3704, 28975, 29892, 385, 14445, 29965, 29892, 263, 2761, 5190, 29892, 322, 385, 15278, 731, 29892, 322, 967, 6210, 338, 17005), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1740120273.3058186, last_token_time=1740120276.6278543, first_scheduled_time=1740120273.3098516, first_token_time=1740120273.3322449, time_in_queue=0.004033088684082031, finished_time=1740120276.6280074, scheduler_time=0.019551593000187495, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.33s/it, est. speed input: 1.80 toks/s, output: 76.95 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RequestOutput(request_id=5, prompt='Explain special relativity', prompt_token_ids=[1, 12027, 7420, 4266, 14215, 537], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' with the help of a thought experiment\\nSpecial relativity is the theory that describes how space and time are affected by motion at high spe lived speeds. It was developed by Albert Einstein in 1905 and is based on two postulates:\\n1. The laws of physics are the same for all observers in inertial reference frames.\\n2. The speed of light is the same for all observers, regardless of their relative motion.\\nTo understand special relativity, we can use a thought experiment, called the \"Einstein\\'s Train Thought Experiment\" or \"Einstein\\'s Two-Arrow Paradox\".\\nImagine an observer on a train that is moving at a constant speed towards you. The train is moving at a speed close to the speed of light and is surrounded by a vacuum. The observer on the train has a light clock, which is a clock made of light beams.\\nAs the train moves towards you, the light beams from the light clock will be coming towards you at different angles, just as the two arrows in the paradox appear to be coming towards you at different angles.\\nHowever, the light beams will still be coming from', token_ids=(411, 278, 1371, 310, 263, 2714, 7639, 13, 24780, 14215, 537, 338, 278, 6368, 393, 16612, 920, 2913, 322, 931, 526, 15201, 491, 10884, 472, 1880, 961, 10600, 961, 5779, 29889, 739, 471, 8906, 491, 10537, 2694, 5465, 297, 29871, 29896, 29929, 29900, 29945, 322, 338, 2729, 373, 1023, 1400, 352, 1078, 29901, 13, 29896, 29889, 450, 14243, 310, 17558, 526, 278, 1021, 363, 599, 5366, 874, 297, 297, 814, 616, 3407, 16608, 29889, 13, 29906, 29889, 450, 6210, 310, 3578, 338, 278, 1021, 363, 599, 5366, 874, 29892, 17126, 310, 1009, 6198, 10884, 29889, 13, 1762, 2274, 4266, 14215, 537, 29892, 591, 508, 671, 263, 2714, 7639, 29892, 2000, 278, 376, 29923, 262, 5465, 29915, 29879, 28186, 498, 1774, 1222, 15362, 29908, 470, 376, 29923, 262, 5465, 29915, 29879, 7803, 29899, 1433, 798, 1459, 912, 29916, 1642, 13, 1888, 22094, 385, 22944, 373, 263, 7945, 393, 338, 8401, 472, 263, 4868, 6210, 7113, 366, 29889, 450, 7945, 338, 8401, 472, 263, 6210, 3802, 304, 278, 6210, 310, 3578, 322, 338, 22047, 491, 263, 11757, 29884, 398, 29889, 450, 22944, 373, 278, 7945, 756, 263, 3578, 12006, 29892, 607, 338, 263, 12006, 1754, 310, 3578, 367, 2232, 29889, 13, 2887, 278, 7945, 16229, 7113, 366, 29892, 278, 3578, 367, 2232, 515, 278, 3578, 12006, 674, 367, 6421, 7113, 366, 472, 1422, 23619, 29892, 925, 408, 278, 1023, 564, 5727, 297, 278, 610, 912, 29916, 2615, 304, 367, 6421, 7113, 366, 472, 1422, 23619, 29889, 13, 17245, 29892, 278, 3578, 367, 2232, 674, 1603, 367, 6421, 515), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1740120276.642153, last_token_time=1740120279.9610023, first_scheduled_time=1740120276.646344, first_token_time=1740120276.6686556, time_in_queue=0.004190921783447266, finished_time=1740120279.9611537, scheduler_time=0.019567144000802728, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\n",
            "\n",
            "\n",
            "Time taken without batching: 15.686182737350464 seconds\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 6/6 [00:03<00:00,  1.81it/s, est. speed input: 13.91 toks/s, output: 190.55 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Explain quantum computing in simple terms\n",
            "Response: .\n",
            "\n",
            "Quantum computing is a type of computing that uses the principles of quantum mechanics to perform calculations. Unlike classical computers that use bits to store and process information, quantum computers use quantum bits, or qubits. Qubits have the ability to exist in multiple states at the same time, allowing quantum computers to perform many calculations simultaneously, making them much faster and more powerful than classical computers.\n",
            "\n",
            "Prompt: Write a poem about artificial intelligence\n",
            "Response: \n",
            "\n",
            "Prompt: Describe the process of photosynthesis\n",
            "Response:  in plants, including the role of chlorophyll and light-dependent and light-independent reactions.\n",
            "Describe the structure and function of mitochondria, including the role of oxidative phosphorylation in cellular respiration.\n",
            "Describe the role of ATP in cellular processes, including energy transfer and storage.\n",
            "Describe the process of transcription and translation, including the role of DNA and RNA in protein synthesis.\n",
            "Describe the structure and function of ribosomes, including the role of RNA in protein synthesis.\n",
            "Describe the process of cellular respiration, including the role of oxygen and electron transport.\n",
            "Describe the process of cell division, including the role of mitosis and meiosis in reproduction and growth.\n",
            "Describe the role of genetic mutations and natural selection in evolution and adaptation.\n",
            "Describe the structure and function of the cell membrane, including the role of lipids and proteins in regulating the movement of molecules in and out of the cell.\n",
            "Describe the structure and function of cellular organelles, including the role of the endoplasmic reticulum in protein synthesis and the role of the lys\n",
            "\n",
            "Prompt: What's the capital of France?\n",
            "Response: \n",
            "The capital of France is Paris.\n",
            "What is the capital of France?\n",
            "The capital of France is Paris.\n",
            "\n",
            "Prompt: How does a CPU work?\n",
            "Response: \n",
            "CPU (Central Processing Unit) is the brain of a computer. It is responsible for executing all the instructions given by the user and the operating system. A CPU is made up of several components, including the control unit, arithmetic logic unit, registers, and memory. The control unit retrieves instructions from memory and decodes them, while the arithmetic logic unit performs mathematical and logical operations. Registers are small, fast memory units that store data and instructions that are currently being used.\n",
            "How does a computer' Click to copyhttps://apnews.com/23b43d31f8f64c049b8f1b20f7e7d77a\n",
            "German court overturns acquittal of man who killed wife\n",
            "BERLIN (AP) — A German court has overturned an acquittal of a man who killed his wife in front of their children and instead convicted him of manslaughter.\n",
            "The regional court in the northern city of Lübeck said Wednesday that the 2015 killing of the woman in front of their two young children was an “especially brutal act” that constituted manslaughter\n",
            "\n",
            "Prompt: Explain special relativity\n",
            "Response:  and its implications in one sentence.\n",
            "\n",
            "\n",
            "\n",
            "Time taken with batching: 3.312987804412842 seconds\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-GUYgR4P3og"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}